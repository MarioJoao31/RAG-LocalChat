# ğŸ“š RAG Document Query Assistant

This project implements a modular Retrieval-Augmented Generation (RAG) pipeline with a user-friendly Streamlit interface. It allows you to ingest, chunk, embed, store, and query documents (PDF, DOCX, TXT) using local or cloud-based embedding models and a vector store.

---

## ğŸš€ Features

- ğŸ” **Document Loader** (PDF, DOCX, TXT)  
- ğŸ§  **Embedding Models** (Hugging Face, Google Gemini, Local)  
- ğŸ§© **Semantic Chunking** with `SemanticChunker`  
- ğŸ—‚ï¸ **Vector Store** via ChromaDB  
- ğŸ’¬ **Natural Language Query Interface** (Streamlit)  

---

## ğŸ§‘â€ğŸ’» How to Run

### 1. ğŸ”§ Install Requirements

```bash
pip install -r requirements.txt
```

### .env

```bash
GOOGLE_API_KEY=your_google_genai_api_key  # if using Google embeddings
```

### ğŸ—ƒï¸ Load and Process Docs

Make sure your input files are in the Docs/ folder.
Run your data prep pipeline (from rag_pipeline.py) once to initialize VectorDataBase/.

### ğŸ–¥ï¸ Launch the App

```bash
streamlit run app.py
```

# ğŸ§  Embedding Options

You can switch between:
 - âœ… Local Hugging Face model (sentence-transformers/all-MiniLM-L6-v2)
 - â˜ï¸ Google Generative AI (embedding-001)
 - ğŸ”— Easily extensible via embedder.py
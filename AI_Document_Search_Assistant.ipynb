{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba02f012",
   "metadata": {},
   "source": [
    "# AI Document Search Assistant\n",
    "## Features\n",
    "- Parses documents from `.txt`, `.pdf`, and `.docx`\n",
    "- Converts them into embeddings\n",
    "- Stores them in ChromaDB\n",
    "- Queries with LangChain integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255d6d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "langchain: 0.3.24\n",
      "chromadb: 0.6.3\n",
      "python-docx: 1.1.2\n",
      "PyMuPDF: Not Installed\n",
      "langchain-community: 0.3.22\n",
      "langchain-openai: 0.3.14\n",
      "sentence-transformers: 4.1.0\n",
      "ipywidgets: 8.1.6\n",
      "tf-keras: 2.19.0\n",
      "langchain-chroma: 0.2.3\n",
      "langchain-huggingface: 0.1.2\n",
      "protobuf: 5.29.4\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "# üì¶ Install necessary packages\n",
    "#!pip install -q langchain chromadb python-docx PyMuPDF\n",
    "#!pip install -U langchain-community\n",
    "#!pip install langchain-openai\n",
    "#!pip install sentence-transformers\n",
    "#!pip install ipywidgets tf-keras\n",
    "#!pip install chromadb langchain-chroma\n",
    "#!pip install langchain-huggingface\n",
    "#!pip install langchain_experimental\n",
    "#!pip install -U langchain-google-genai\n",
    "!pip uninstall protobuf\n",
    "!pip install protobuf==6.31\n",
    "\n",
    "\n",
    "\n",
    "installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "\n",
    "packages_to_check = [\n",
    "    \"langchain\",\n",
    "    \"chromadb\",\n",
    "    \"python-docx\",\n",
    "    \"PyMuPDF\",\n",
    "    \"langchain-community\",\n",
    "    \"langchain-openai\",\n",
    "    \"sentence-transformers\",\n",
    "    \"ipywidgets\",\n",
    "    \"tf-keras\",\n",
    "    \"langchain-chroma\",\n",
    "    \"langchain-huggingface\",\n",
    "    \"protobuf\"\n",
    "]\n",
    "\n",
    "for package in packages_to_check:\n",
    "    version = installed_packages.get(package, \"Not Installed\")\n",
    "    print(f\"{package}: {version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ea759",
   "metadata": {},
   "source": [
    "### Document Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4d8bec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Document Parsing\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import docx\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    return \"\\n\".join([page.get_text() for page in doc])\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def parse_documents(folder):\n",
    "    docs = []\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            path = os.path.join(root, file)\n",
    "            ext = file.lower().split('.')[-1]\n",
    "            try:\n",
    "                if ext == 'txt':\n",
    "                    text = read_txt(path)\n",
    "                elif ext == 'pdf':\n",
    "                    text = read_pdf(path)\n",
    "                elif ext == 'docx':\n",
    "                    text = read_docx(path)\n",
    "                else:\n",
    "                    continue\n",
    "                docs.append({'text': text, 'path': path})\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read {file}: {e}\")\n",
    "    return docs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12272f30",
   "metadata": {},
   "source": [
    "### Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8d87e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Embedding Generation and Storage with ChromaDB\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "# Set the API key globally\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "OpenAIEmbeddings.api_key = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "def store_embeddings(docs, persist_directory=\"VectorDB\"):\n",
    "\n",
    "    vector_store = Chroma(\n",
    "    collection_name=\"my_collection\",\n",
    "    embedding_function=SentenceTransformer(\"all-MiniLM-L6-v2\"),\n",
    "    #persist_directory=persist_directory,\n",
    "    )\n",
    "\n",
    "    #create an list of texts \n",
    "    texts = [doc['text'] for doc in docs]\n",
    "    \n",
    "    #add path to the metadata\n",
    "    metadatas = [{\"source\": doc['path']} for doc in docs]\n",
    "\n",
    "    # Generate unique IDs for each document\n",
    "    ids = [f\"doc_{i}\" for i in range(len(texts))]\n",
    "\n",
    "    # can create the embeddings using openAI or Chromedb hugginFace\n",
    "    #embeddings = OpenAIEmbeddings().embed_documents(texts)\n",
    "    embeddings = SentenceTransformer(\"all-MiniLM-L6-v2\").encode(texts)\n",
    "\n",
    "    # creating embeddings using huggingface and chunking them \n",
    "\n",
    "\n",
    "    print(type(embeddings))\n",
    "    \n",
    "    # Add documents with precomputed embeddings\n",
    "    \n",
    "    vector_store.add_documents(\n",
    "        documents=[Document(page_content=text, metadata=metadata) for text, metadata in zip(texts, metadatas)],\n",
    "        embeddings=embeddings,\n",
    "        ids=ids\n",
    "    )\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b7f224d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Querying with LangChain\n",
    "def query_db(query, db):\n",
    "\n",
    "    results = db.similarity_search(query)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da80641c",
   "metadata": {},
   "source": [
    "# Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "089f23d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SentenceTransformer' object has no attribute 'embed_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m docs = parse_documents(\u001b[33m\"\u001b[39m\u001b[33mDocs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Store them in vector DB\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m db = \u001b[43mstore_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Query the assistant\u001b[39;00m\n\u001b[32m      6\u001b[39m results = query_db(\u001b[33m\"\u001b[39m\u001b[33mWhat fruits and vegies give most energy\u001b[39m\u001b[33m\"\u001b[39m, db)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mstore_embeddings\u001b[39m\u001b[34m(docs, persist_directory)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(embeddings))\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Add documents with precomputed embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m vector_store\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Manuchupamos/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:288\u001b[39m, in \u001b[36mVectorStore.add_documents\u001b[39m\u001b[34m(self, documents, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m     texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    287\u001b[39m     metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m msg = (\n\u001b[32m    290\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    291\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    292\u001b[39m )\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Manuchupamos/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:277\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m(texts)\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    279\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    280\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    281\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Manuchupamos/lib/python3.11/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'SentenceTransformer' object has no attribute 'embed_documents'"
     ]
    }
   ],
   "source": [
    "# Parse documents from a folder\n",
    "docs = parse_documents(\"Docs\")\n",
    "# Store them in vector DB\n",
    "db = store_embeddings(docs)\n",
    "# Query the assistant\n",
    "results = query_db(\"What fruits and vegies give most energy\", db)\n",
    "for r in results:\n",
    "    print(r.metadata[\"source\"], \"\\n\", r.page_content[:200], \"...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
